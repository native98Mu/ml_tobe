{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd0e5d5a545d47a452400ed0f023be4625769e773074f6a65de0103e9cc8585fc80",
   "display_name": "Python 3.7.10 64-bit ('paddle_quantum_env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 线性系统求解\n",
    "$Ax = b  $\n",
    "$$A = c_0A_0+c_1A_1+c_2A_2=\\mathbb{I}+0.2X_0Z_1+0.2X_0$$\n",
    "$$|b\\rangle = U|0\\rangle = H_0H_1H_2|0\\rangle$$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import paddle \n",
    "from paddle import matmul\n",
    "from paddle_quantum.circuit import UAnsatz\n",
    "from paddle_quantum.utils import random_pauli_str_generator, pauli_str_to_matrix, dagger\n",
    "#参数设置\n",
    "num_qubit = 3  \n",
    "num_shots = 10**6 # Number of quantum measurements\n",
    "tot_qubits = num_qubit+1 # Addition of an ancillary qubit\n",
    "ancilla_idx = num_qubit #Index of the ancillary qubit (last position)\n",
    "ITR = 80 #训练迭代次数\n",
    "LR = 0.5 #学习速率\n",
    "seed = 0 # Seed for random number generator\n",
    "# q_delta = 0.001\n",
    "# q_delta = paddle.to_tensor(q_delta,dtype = 'complex128')\n",
    "#构造cz gate\n",
    "a= np.array([np.pi/2])\n",
    "cz_theta = paddle.to_tensor(a)\n",
    "#coefficient of A=c_0 A_0 + c_1 A_1 ...\n",
    "c = np.array([1,0.2,0.2])\n",
    "c = paddle.to_tensor(c,dtype = 'complex128')\n"
   ]
  },
  {
   "source": [
    "## 各门电路定义\n",
    "U操作: $U|0\\rangle = |b\\rangle$  \n",
    "\n",
    "A操作: $A = \\mathbb{I}+0.2X_0Z_1+0.2X_0$\n",
    "\n",
    "要训练的量子网络V $V|0\\rangle = |x\\rangle$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义门电路U U|0> = |b>\n",
    "def U_b(cir):\n",
    "    for idx in range(num_qubit):\n",
    "        cir.h(idx)\n",
    "# 定义受控酉矩阵\n",
    "def CA(cir,idx):\n",
    "    if idx == 0:\n",
    "        # Identity operation\n",
    "        None\n",
    "    elif idx == 1:\n",
    "        cir.cnot([ancilla_idx,0])\n",
    "        #cz gate\n",
    "        cir.s(ancilla_idx)\n",
    "        cir.s(ancilla_idx)\n",
    "        cir.s(ancilla_idx)\n",
    "        cir.cnot([ancilla_idx,1])\n",
    "        cir.rz(cz_theta,1)\n",
    "        cir.cnot([ancilla_idx,1])\n",
    "        cir.rz(-cz_theta,1)\n",
    "    elif idx == 2:\n",
    "        cir.cnot([ancilla_idx,0])\n",
    "#定义变分量子电路 使得|x> = V|0>\n",
    "def variational_block(cir,theta):\n",
    "    '''\n",
    "    QNN\n",
    "    '''\n",
    "    #第一层是给除附加位外所有量子比特施加h门\n",
    "    for idx in range(num_qubit):\n",
    "        cir.h(idx)\n",
    "    #单层的全y门\n",
    "    for idx, element in enumerate(theta):\n",
    "        cir.ry(element,idx)\n"
   ]
  },
  {
   "source": [
    "## hadamard test\n",
    "根据hadamard test电路要求，按顺序放好构建的门序列。  \n",
    "\n",
    "获取系统最终状态里附加位为$|0\\rangle$的概率（由于paddle_quantum中没有可以直接测量系统中某条电路状态的方法，故根据测量原理，构建投影算子M，计算辅助位为$|0\\rangle$概率）\n",
    "\n",
    "根据hadamard test算法，只需要对辅助比特进行计算基测量，测量得到结果为$|0\\rangle$的概率与两量子态内积有：$Re(\\langle\\phi_1|\\phi\\rangle = 2*p_0 -1$ 类似的，虚数部分需要添加相位变换s gate，相同的推导可以得到虚数部分$\\operatorname{Im}\\left(\\left\\langle\\phi_{1} \\mid \\phi_{2}\\right\\rangle\\right)=2 p_{0}-1$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hadamard test\n",
    "def local_hadamard_test(theta,l = None,lp=None,j = None,part = None):\n",
    "    # print('test: l,lp,j', l,lp,j)\n",
    "    #初始化电路\n",
    "    cir = UAnsatz(tot_qubits)#需要附加位\n",
    "    #作用在附加位上的H门\n",
    "    cir.h(ancilla_idx)\n",
    "    #如果要计算的是虚数部，附加位还需要加一个相位门\n",
    "    #phase gate\n",
    "    if part == 'Im' or part == 'im':\n",
    "        cir.s(ancilla_idx)\n",
    "    #生成一个向量｜x>\n",
    "    variational_block(cir,theta)\n",
    "    #使用A_l\n",
    "    CA(cir,l)\n",
    "    #添加U_b\n",
    "    U_b(cir)\n",
    "    #添加受控z\n",
    "    if j != -1:\n",
    "        #cz gate\n",
    "        cir.s(ancilla_idx)\n",
    "        cir.s(ancilla_idx)\n",
    "        cir.s(ancilla_idx)\n",
    "        cir.cnot([ancilla_idx,j])\n",
    "        cir.rz(cz_theta,j)\n",
    "        cir.cnot([ancilla_idx,j])\n",
    "        cir.rz(-cz_theta,j)\n",
    "    #添加U\n",
    "    U_b(cir)\n",
    "    #contralled gate为A的共轭转置\n",
    "    CA(cir,lp)\n",
    "    #辅助位第二个hadamard门\n",
    "    cir.h(ancilla_idx)\n",
    "    #对辅助比特为｜0>进行观测\n",
    "    fi_state = cir.run_state_vector()\n",
    "    state = paddle.reshape(fi_state, shape=(2**tot_qubits, 1))\n",
    "    # print(state.shape)\n",
    "    M_0 = np.array([[1,0],[0,0]])\n",
    "    Id = np.identity(2)\n",
    "    M = np.kron(np.kron(Id,Id),Id)\n",
    "    M = np.kron(M,M_0)\n",
    "    M = paddle.to_tensor(M)\n",
    "    #计算辅助位｜0>的概率\n",
    "    M = paddle.matmul(dagger(M), M)\n",
    "    p0 = paddle.matmul(paddle.matmul(dagger(state), M), state) \n",
    "    b2t = paddle.to_tensor(2.0)\n",
    "    p0 = paddle.matmul(b2t,p0)\n",
    "    b1t = paddle.to_tensor(1.0)\n",
    "    po = paddle.subtract(p0,b1t)\n",
    "    # print(p0)\n",
    "    return p0"
   ]
  },
  {
   "source": [
    "## local cost function中重要系数$\\mu$计算\n",
    "根据论文中描述，可以知道局部cost function需使用hadamard test计算。$\\delta_{l l^{\\prime}}^{(j)}=\\left\\langle\\mathbf{0}\\left|V^{\\dagger} A_{l^{\\prime}}^{\\dagger} U\\left(\\left|0_{j}\\right\\rangle\\left\\langle 0_{j}\\right| \\otimes \\mathbb{I}_{\\bar{J}}\\right) U^{\\dagger} A_{l} V\\right| \\mathbf{0}\\right\\rangle$，其中$\\left|0_{j}\\right\\rangle\\left\\langle 0_{j}\\right|=\\left(\\mathbb{I}_{j},Z_j)/2\\right.$，所以有\n",
    "$\\delta_{l l^{\\prime}}^{(j)}=\\beta_{l l^{\\prime}}+\\left\\langle\\mathbf{0}\\left|V^{\\dagger} A_{l^{\\prime}}^{\\dagger} U\\left(Z_{j} \\otimes \\mathbb{I}_{\\bar{\\jmath}}\\right) U^{\\dagger} A_{l} V\\right| \\mathbf{0}\\right\\rangle$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算local cost function的系数\n",
    "def mu(theta,l = None,lp=None,j=None):\n",
    "    # print('mu: l,lp,j= ',l,lp,j)\n",
    "    mu_real = local_hadamard_test(theta,l = l,lp = lp,j = j,part='Re')\n",
    "    mu_imag = local_hadamard_test(theta,l = l, lp = lp, j = j, part= 'Im')\n",
    "    jj = paddle.to_tensor([1j],dtype = 'complex128')\n",
    "    muval = mu_real+ mu_imag * jj\n",
    "    return muval"
   ]
  },
  {
   "source": [
    "## local cost function 计算\n",
    "首先对$\\langle x|A^{\\dagger}A|x\\rangle$的评估  \n",
    "接着利用hadamard test算得的mu，求和计算loss值  \n",
    "求loss值的过程中，三层循环相当于是对每个$A_l,A^{\\dagger}_l,Z_j$做了一遍测量 即要做$3*3*3 = 27$次"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 归一化 先计算 <x|A'A|x> 分母\n",
    "def psi_norm(theta):\n",
    "    '''Returns the normalization constant <psi|psi>'''\n",
    "    norm = 0\n",
    "    norm = paddle.to_tensor(norm,dtype = 'complex128')\n",
    "    for l in range(c.size):\n",
    "        for lp in range(c.size):\n",
    "            norm = norm+c[l]*paddle.conj(c[lp])*mu(theta,l,lp,-1)\n",
    "    return paddle.abs(norm)\n",
    "#计算cost function\n",
    "def local_cost(theta):\n",
    "    #当A｜x>和|b>同方向时，该值越接近0\n",
    "    mu_sum = 0\n",
    "    mu_sum = paddle.to_tensor(mu_sum,dtype = 'complex128')\n",
    "    #对每一个A0-2\n",
    "    for l in range(0, c.size):\n",
    "        for lp in range(0, c.size):\n",
    "            for j in range(0, num_qubit):\n",
    "                mu_sum = mu_sum + c[l] * paddle.conj(c[lp]) * mu(theta, l, lp, j)\n",
    "    \n",
    "    mu_sum = paddle.abs(mu_sum)\n",
    "    m1 = num_qubit * psi_norm(theta)\n",
    "    result = 0.5 - 0.5 * mu_sum / m1\n",
    "    # Cost function C_L\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta = np.random.uniform(0,2*np.pi,num_qubit)\n",
    "# theta = paddle.to_tensor(theta)\n",
    "# # norm = psi_norm(theta)\n",
    "# # print(norm)\n",
    "# loss =local_cost(theta)\n",
    "# print('%.10f' %loss)"
   ]
  },
  {
   "source": [
    "## 优化--梯度下降\n",
    "利用adam优化器，对角度参数$theta$进行优化，使得loss值不断减小，即$V|0\\rangle - |b\\rangle$趋于0  \n",
    "要调用优化器时，涉及到的计算参数必须是paddle形式的计算 如果是numpy形式无法处理"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vqlsOpt(paddle.nn.Layer):\n",
    "    def __init__(self, shape, dtype='float64'):\n",
    "        super(vqlsOpt, self).__init__()\n",
    "        # 初始化 theta 参数列表，并用 [0, 2*pi] 的均匀分布来填充初始值\n",
    "        self.theta = self.create_parameter(shape=shape,default_initializer=paddle.nn.initializer.Uniform(low=0.0, high=2*np.pi), dtype=dtype, is_bias=False)\n",
    "        \n",
    "    # 定义损失函数和前向传播机制\n",
    "    def forward(self):\n",
    "        #初始化电路\n",
    "        cir_v = UAnsatz(num_qubit)\n",
    "        U = variational_block(cir_v,self.theta)\n",
    "        #计算损失函数\n",
    "        loss = local_cost(self.theta)\n",
    "        return loss"
   ]
  },
  {
   "source": [
    "## 迭代优化\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "iter: 1   loss: 0.0832684\n",
      "iter: 2   loss: 0.0381729\n",
      "iter: 3   loss: 0.0161175\n",
      "iter: 4   loss: 0.0172738\n",
      "iter: 5   loss: 0.0200259\n",
      "iter: 6   loss: 0.0158604\n",
      "iter: 7   loss: 0.0109704\n",
      "iter: 8   loss: 0.0100892\n",
      "iter: 9   loss: 0.0116899\n",
      "iter: 10   loss: 0.0124964\n",
      "iter: 11   loss: 0.0111305\n",
      "iter: 12   loss: 0.0081296\n",
      "iter: 13   loss: 0.0049659\n",
      "iter: 14   loss: 0.0029259\n",
      "iter: 15   loss: 0.0023992\n",
      "iter: 16   loss: 0.0030672\n",
      "iter: 17   loss: 0.0043916\n",
      "iter: 18   loss: 0.0057376\n",
      "iter: 19   loss: 0.0063944\n",
      "iter: 20   loss: 0.0058587\n",
      "iter: 21   loss: 0.0041897\n",
      "iter: 22   loss: 0.0020685\n",
      "iter: 23   loss: 0.0004625\n",
      "iter: 24   loss: 0.0000577\n",
      "iter: 25   loss: 0.0008150\n",
      "iter: 26   loss: 0.0020157\n",
      "iter: 27   loss: 0.0027936\n",
      "iter: 28   loss: 0.0027017\n",
      "iter: 29   loss: 0.0019028\n",
      "iter: 30   loss: 0.0009476\n",
      "iter: 31   loss: 0.0003611\n",
      "iter: 32   loss: 0.0003199\n",
      "iter: 33   loss: 0.0006217\n",
      "iter: 34   loss: 0.0009214\n",
      "iter: 35   loss: 0.0010023\n",
      "iter: 36   loss: 0.0008727\n",
      "iter: 37   loss: 0.0006715\n",
      "iter: 38   loss: 0.0005205\n",
      "iter: 39   loss: 0.0004479\n",
      "iter: 40   loss: 0.0004116\n",
      "iter: 41   loss: 0.0003638\n",
      "iter: 42   loss: 0.0002960\n",
      "iter: 43   loss: 0.0002391\n",
      "iter: 44   loss: 0.0002313\n",
      "iter: 45   loss: 0.0002790\n",
      "iter: 46   loss: 0.0003393\n",
      "iter: 47   loss: 0.0003461\n",
      "iter: 48   loss: 0.0002660\n",
      "iter: 49   loss: 0.0001355\n",
      "iter: 50   loss: 0.0000377\n",
      "iter: 51   loss: 0.0000330\n",
      "iter: 52   loss: 0.0001083\n",
      "iter: 53   loss: 0.0001900\n",
      "iter: 54   loss: 0.0002083\n",
      "iter: 55   loss: 0.0001510\n",
      "iter: 56   loss: 0.0000650\n",
      "iter: 57   loss: 0.0000108\n",
      "iter: 58   loss: 0.0000149\n",
      "iter: 59   loss: 0.0000559\n",
      "iter: 60   loss: 0.0000911\n",
      "iter: 61   loss: 0.0000930\n",
      "iter: 62   loss: 0.0000659\n",
      "iter: 63   loss: 0.0000345\n",
      "iter: 64   loss: 0.0000192\n",
      "iter: 65   loss: 0.0000218\n",
      "iter: 66   loss: 0.0000306\n",
      "iter: 67   loss: 0.0000339\n",
      "iter: 68   loss: 0.0000301\n",
      "iter: 69   loss: 0.0000251\n",
      "iter: 70   loss: 0.0000232\n",
      "iter: 71   loss: 0.0000226\n",
      "iter: 72   loss: 0.0000189\n",
      "iter: 73   loss: 0.0000121\n",
      "iter: 74   loss: 0.0000071\n",
      "iter: 75   loss: 0.0000083\n",
      "iter: 76   loss: 0.0000141\n",
      "iter: 77   loss: 0.0000179\n",
      "iter: 78   loss: 0.0000151\n",
      "iter: 79   loss: 0.0000076\n",
      "iter: 80   loss: 0.0000015\n"
     ]
    }
   ],
   "source": [
    "# 定义网络维度\n",
    "paddle.seed(seed)\n",
    "vqls = vqlsOpt(shape = [3])\n",
    "# 一般来说，我们利用Adam优化器来获得相对好的收敛，当然你可以改成SGD或者是RMS prop.\n",
    "opt = paddle.optimizer.Adam(learning_rate = LR, parameters = vqls.parameters())    \n",
    "# 优化循环\n",
    "for itr in range(1,ITR+1):\n",
    "    # 前向传播计算损失函数\n",
    "    loss = vqls()\n",
    "    # 反向传播极小化损失函数\n",
    "    loss.backward()\n",
    "    opt.minimize(loss)\n",
    "    opt.clear_grad()\n",
    "    print('iter:', itr, '  loss: %.7f' % loss.numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parameter containing:\nTensor(shape=[3], dtype=float64, place=CPUPlace, stop_gradient=False,\n       [0.00705842, 0.32586962, 6.28196748])\n"
     ]
    }
   ],
   "source": [
    "print(vqls.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-0.29066532+0.j -0.29031155+0.j -0.40503258+0.j -0.40453962+0.j\n -0.29272423+0.j -0.29236796+0.j -0.40790161+0.j -0.40740516+0.j]\n"
     ]
    }
   ],
   "source": [
    "#验证\n",
    "weight = vqls.theta\n",
    "cir_test = UAnsatz(num_qubit)\n",
    "def prepare_x(theta):\n",
    "    a = variational_block(cir_test,theta)\n",
    "    # U= cir_test.U.numpy()\n",
    "    # print(U)\n",
    "    x_0 = cir_test.run_state_vector()\n",
    "    print(x_0.numpy())\n",
    "prepare_x(weight)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "source": [
    "## 量子电路结果和经典结果比较\n",
    "获得优化器中最终的$theta$参数列表，重新初始化量子门电路$V$，带入参数$theta$，观察计算结果"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 传统numpy方法计算\n",
    "$Ax=b$ 计算得到x的真实值 用于和量子方法进行对比"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A = \n",
      " [[1.  0.  0.  0.  0.4 0.  0.  0. ]\n",
      " [0.  1.  0.  0.  0.  0.4 0.  0. ]\n",
      " [0.  0.  1.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  1.  0.  0.  0.  0. ]\n",
      " [0.4 0.  0.  0.  1.  0.  0.  0. ]\n",
      " [0.  0.4 0.  0.  0.  1.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.  1.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.  0.  1. ]]\n",
      "b = \n",
      " [0.35355339 0.35355339 0.35355339 0.35355339 0.35355339 0.35355339\n",
      " 0.35355339 0.35355339]\n",
      "x = \n",
      " [0.25253814 0.25253814 0.35355339 0.35355339 0.25253814 0.25253814\n",
      " 0.35355339 0.35355339]\n",
      "/usr/local/anaconda3/envs/paddle_quantum_env/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "Id = np.identity(2)\n",
    "Z = np.array([[1, 0], [0, -1]])\n",
    "X = np.array([[0, 1], [1, 0]])\n",
    "ct = [1,0.2,0.2]\n",
    "A_0 = np.identity(8)\n",
    "A_1 = np.kron(np.kron(X, Z), Id)\n",
    "A_2 = np.kron(np.kron(X, Id), Id)\n",
    "\n",
    "A_num = ct[0] * A_0 + ct[1] * A_1 + ct[2] * A_2\n",
    "b = np.ones(8) / np.sqrt(8)\n",
    "print(\"A = \\n\", A_num)\n",
    "print(\"b = \\n\", b)\n",
    "A_inv = np.linalg.inv(A_num)\n",
    "x_real = np.dot(A_inv, b)\n",
    "print('x = \\n',x_real)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3\n",
      "Tensor(shape=[1], dtype=complex64, place=CPUPlace, stop_gradient=True,\n",
      "       [1j])\n",
      "1\n",
      "/usr/local/anaconda3/envs/paddle_quantum_env/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "a = [5.0,8.3,2.5]\n",
    "\n",
    "b = paddle.to_tensor(a)\n",
    "jj = paddle.to_tensor([1j])\n",
    "print(b.size)\n",
    "print(jj)\n",
    "print(jj.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tensor(shape=[4, 4], dtype=float64, place=CPUPlace, stop_gradient=True,\n       [[1., 0., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]])\n(4,)\n(4, 1)\n(1, 4)\nTensor(shape=[1, 1], dtype=float64, place=CPUPlace, stop_gradient=True,\n       [[5.]])\nTensor(shape=[1, 1], dtype=float64, place=CPUPlace, stop_gradient=True,\n       [[20.]])\n"
     ]
    }
   ],
   "source": [
    "a1 = np.array([[1,0],[0,0]])\n",
    "a2 = np.identity(2)\n",
    "p = np.kron(a1,a2)\n",
    "p = paddle.to_tensor(p)\n",
    "#计算辅助位｜0>的概率\n",
    "p = paddle.matmul(dagger(p), p)\n",
    "print(p)\n",
    "a = np.array([1, 2, 3,4])\n",
    "b = a.reshape(-1, 1)\n",
    "a1=paddle.to_tensor(a,dtype = 'float64')\n",
    "print(a.shape)\n",
    "b1 = paddle.to_tensor(b,dtype = 'float64')\n",
    "print(b.shape)\n",
    "a = b.reshape(1,4)\n",
    "a1=paddle.to_tensor(a,dtype = 'float64')\n",
    "print(a.shape)\n",
    "iouy = paddle.matmul(p,dagger(p))\n",
    "iou = paddle.matmul(paddle.matmul(a1,iouy),b1)\n",
    "print(iou)\n",
    "\n",
    "print(iou*4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tensor(shape=[1], dtype=complex128, place=CPUPlace, stop_gradient=True,\n",
      "       [(2+0j)])\n",
      "Tensor(shape=[1], dtype=complex128, place=CPUPlace, stop_gradient=True,\n",
      "       [1j])\n",
      "Tensor(shape=[1], dtype=complex128, place=CPUPlace, stop_gradient=True,\n",
      "       [2j])\n",
      "/usr/local/anaconda3/envs/paddle_quantum_env/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "ooo = 2 \n",
    "oo = paddle.to_tensor(ooo,dtype = 'complex128')\n",
    "print(oo)\n",
    "jj = paddle.to_tensor([1j],dtype = 'complex128')\n",
    "print(jj)\n",
    "print(oo*jj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}